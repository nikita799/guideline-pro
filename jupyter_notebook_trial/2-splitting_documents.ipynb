{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1471fc12-6ea1-4a54-84aa-028544872d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "open_api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "md_path = Path(\"data/t2d_guideline_ee/20_normalized_md/2-tuubi-diabeedi-diagnostika-ravi.md\")\n",
    "\n",
    "text = md_path.read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad016ed-7272-4b05-bedc-b7660ae38218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context aware splitting\n",
    "headers_to_split_on  = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22aa7efd-0916-4044-bf07-646c38edcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b67ec46-8643-497e-a554-e7d0d990aa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. **Prediabeediga patsient suunake eluviisisekkumise intensiivprogrammi.**\\nTugev positiivne soovitus, m√µ√µdukas t√µendatuse aste\\n2. **Prediabeediga patsiendil √§rge metformiinravi pigem kasutage.**\\nN√µrk negatiivne soovitus, m√µ√µdukas t√µendatuse aste'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be added to metadata or something\n",
    "md_header_splits[5].metadata\n",
    "md_header_splits[5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0143fc76-84a1-4e39-9a47-a69ccf2234c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model = \"gpt-5.2\",\n",
    "    input = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You answer questions based on the user query and result from vector database. Answer without any formatting. Do not add any data that is not in the vector database result.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Keda peaks s√µeluma diabeedi osas?\n",
    "            \n",
    "            Vector database result: {md_header_splits[3].page_content}\"\"\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f9df6fc-6ed7-494a-8766-37920cc38b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diabeedi s√µelumist tuleks teha v√µi kaaluda j√§rgmistel juhtudel:\\n\\n1) T√§iskasvanud enne 45. eluaastat, kui esineb √ºlekaal ja/v√µi rasvumine (KMI ‚â• 25 kg/m¬≤) ning lisaks √ºks v√µi mitu riskitegurit:\\n- esimese astme sugulasel on 2. t√º√ºpi diabeet\\n- suure riskiga rass / etniline taust\\n- anamneesis SVH\\n- h√ºpertensioon (verer√µhk ‚â• 140/90 mmHg v√µi tarvitab verer√µhuravimit)\\n- HDL-Chol < 0,90 mmol/l v√µi trigl√ºtseriidid > 2,82 mmol/l\\n- pol√ºts√ºstiliste munasarjade s√ºndroom\\n- v√§hene f√º√ºsiline aktiivsus\\n- viited insuliiniresistentsusele\\n\\n2) Prediabeediga patsiendid (HbA1c 6,0%‚Äì6,4% ehk 42‚Äì47 mmol/mol ja/v√µi IFG v√µi IGT): gl√ºkoosi m√µ√µtmine v√§hemalt kord aastas\\n\\n3) Gestatsioonidiabeedi diagnoosiga naised: v√§hemalt iga 3 aasta tagant\\n\\n4) HIV-iga patsiendid\\n\\n5) K√µik teised t√§iskasvanud alates 45. eluaastast; kui gl√ºkoos on normis, korrata s√µeluuringut v√§hemalt iga 3 aasta tagant'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess if the length of the splits is good enough or should be shorter. Should run trough tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c63944f1-e244-4628-9fb4-746165895747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def build_breadcrumbs(md: dict) -> str:\n",
    "    \"\"\"\n",
    "    Merge Header 1 / Header 2 / Header 3 (or h1/h2/h3)\n",
    "    into a single breadcrumb string.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        md.get(\"Header 1\") or md.get(\"h1\"),\n",
    "        md.get(\"Header 2\") or md.get(\"h2\"),\n",
    "        md.get(\"Header 3\") or md.get(\"h3\"),\n",
    "    ]\n",
    "    \n",
    "    parts = [\n",
    "        p.strip()\n",
    "        for p in candidates\n",
    "        if p and isinstance(p, str) and p.strip()\n",
    "    ]\n",
    "    \n",
    "    return \" > \".join(parts)\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def enrich_splits(\n",
    "    splits: list[Document],\n",
    "    *,\n",
    "    doc_id: str,\n",
    "    version_id: str,\n",
    "    published_year: int,\n",
    "    language: str,\n",
    "    source_path: str,\n",
    ") -> list[Document]:\n",
    "    enriched = []\n",
    "\n",
    "    for i, d in enumerate(splits):\n",
    "        md = dict(d.metadata or {})\n",
    "\n",
    "        # Build breadcrumbs from headers\n",
    "        breadcrumbs = build_breadcrumbs(md)\n",
    "\n",
    "        # Reset metadata to only what you want\n",
    "        md = {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"version_id\": version_id,\n",
    "            \"published_year\": published_year,\n",
    "            \"language\": language,\n",
    "            \"source_path\": source_path,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_id\": f\"{doc_id}::{version_id}::chunk_{i:04d}\",\n",
    "            \"text_hash\": sha256_text(d.page_content),\n",
    "            \"breadcrumbs\": breadcrumbs,\n",
    "        }\n",
    "\n",
    "        enriched.append(\n",
    "            Document(\n",
    "                page_content=d.page_content,\n",
    "                metadata=md,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return enriched\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815b76f-b12e-4696-bde0-4a95a8306fea",
   "metadata": {},
   "source": [
    "### Aim to have the output like\n",
    "[Source: 2. t√º√ºpi diabeedi diagnostika ja ravi (RJ-E/51.1-2021, 2021)]\n",
    "[Section: Ravijuhendi soovituste loetelu > Ravi eesm√§rkv√§√§rtused]\n",
    "[Chunk: recommendation #23 | Strength: tugev | Evidence: madal]\n",
    "<chunk text here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9e21f0f-674e-4fd7-9add-b30978218d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = enrich_splits(\n",
    "    md_header_splits,\n",
    "    doc_id=\"t2d_guideline_ee\",\n",
    "    version_id=\"RJ-E_51.1-2021\",\n",
    "    published_year=2021,\n",
    "    language=\"et\",\n",
    "    source_path=\"docs/t2d_guideline_ee/v2021_RJ-E-51.1-2021/10_canonical_md/canonical.md\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c77e8ea7-1fe6-4a7d-8af6-cde9ba5de57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 't2d_guideline_ee', 'version_id': 'RJ-E_51.1-2021', 'published_year': 2021, 'language': 'et', 'source_path': 'docs/t2d_guideline_ee/v2021_RJ-E-51.1-2021/10_canonical_md/canonical.md', 'chunk_index': 3, 'chunk_id': 't2d_guideline_ee::RJ-E_51.1-2021::chunk_0003', 'text_hash': 'e8a2d4b6b3fa0756898f79b6b9dbebc37230c555da9382d4a8ecb8738d2830b2', 'breadcrumbs': 'Prediabeet ja 2. t√º√ºpi diabeedi diagnoosimine > Prediabeedi ja diabeedi s√µeluuring s√ºmptomiteta t√§iskasvanutel (2)'}, page_content='1) S√µelumist tuleks kaaluda t√§iskasvanutel enne 45. eluaastat, kui esineb √ºlekaal ja/v√µi rasvumine (kehamassiindeks ehk KMI ‚â• 25 kg/m¬≤) ning lisaks sellele √ºks v√µi mitu j√§rgmist riskitegurit:\\n- esimese astme sugulasel on diagnoositud 2. t√º√ºpi diabeet\\n- suure riskiga rass / etniline taust\\n- anamneesis SVH\\n- h√ºpertensioon (verer√µhk ‚â• 140/90 mmHg v√µi tarvitab verer√µhuravimit)\\n- HDL-Chol < 0,90 mmol/l v√µi trigl√ºtseriidid > 2,82 mmol/l\\n- pol√ºts√ºstiliste munasarjade s√ºndroom\\n- v√§hene f√º√ºsiline aktiivsus\\n- viited insuliiniresistentsusele  \\n2) prediabeediga patsientidel (HbA1c vahemikus 6,0%‚Äì6,4% v√µi 42‚Äì47 mmol/mol ja/v√µi esineb IFG v√µi IGT) tuleks vere gl√ºkoosisisaldust m√µ√µta v√§hemalt √ºks kord aastas  \\n3) gestatsioonidiabeedi diagnoosiga naistel v√§hemalt iga kolme aasta tagant  \\n4) HIV-iga patsientidel  \\n5) K√µigil teistel t√§iskasvanutel tuleks s√µelumist alustada alates 45. eluaastast. Kui vere gl√ºkoosisisaldus j√§√§b normaalsesse vahemikku, tuleks s√µeluuringut korrata v√§hemalt iga kolme aasta tagant.  \\n---')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2b385df-03fd-4c1e-85fc-e0177bfb5019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0000 3072\n",
      "1 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0001 3072\n",
      "2 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0002 3072\n",
      "3 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0003 3072\n",
      "4 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0004 3072\n",
      "5 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0005 3072\n",
      "6 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0006 3072\n",
      "7 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0007 3072\n",
      "8 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0008 3072\n",
      "9 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0009 3072\n",
      "10 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0010 3072\n",
      "11 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0011 3072\n",
      "12 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0012 3072\n",
      "13 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0013 3072\n",
      "14 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0014 3072\n",
      "15 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0015 3072\n",
      "16 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0016 3072\n",
      "17 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0017 3072\n",
      "18 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0018 3072\n",
      "19 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0019 3072\n",
      "20 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0020 3072\n",
      "21 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0021 3072\n",
      "22 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0022 3072\n",
      "23 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0023 3072\n",
      "24 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0024 3072\n",
      "25 t2d_guideline_ee::RJ-E_51.1-2021::chunk_0025 3072\n",
      "Unique embedding lengths: [3072]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # expects OPENAI_API_KEY in env\n",
    "\n",
    "def embed_and_report_lengths(\n",
    "    splits,\n",
    "    model: str = \"text-embedding-3-large\",\n",
    "    batch_size: int = 64,\n",
    "    dimensions: int | None = None,   # None => model default\n",
    "):\n",
    "    \"\"\"\n",
    "    For each split (LangChain Document-like object with .page_content and .metadata),\n",
    "    call OpenAI embeddings and print len(embedding) for each split.\n",
    "\n",
    "    Returns: list of dicts with chunk_id (if present), chunk_index (if present), and embedding_length.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Prepare texts\n",
    "    texts = [d.page_content for d in splits]\n",
    "\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[start:start + batch_size]\n",
    "\n",
    "        # Build request payload\n",
    "        req = {\"model\": model, \"input\": batch_texts, \"encoding_format\": \"float\"}\n",
    "        if dimensions is not None:\n",
    "            req[\"dimensions\"] = dimensions  # supported for text-embedding-3* models :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "        resp = client.embeddings.create(**req)\n",
    "\n",
    "        # resp.data is aligned with inputs order\n",
    "        for j, item in enumerate(resp.data):\n",
    "            doc = splits[start + j]\n",
    "            emb = item.embedding\n",
    "            emb_len = len(emb)\n",
    "\n",
    "            chunk_id = (doc.metadata or {}).get(\"chunk_id\")\n",
    "            chunk_index = (doc.metadata or {}).get(\"chunk_index\", start + j)\n",
    "\n",
    "            results.append({\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"embedding_length\": emb_len,\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "lengths = embed_and_report_lengths(\n",
    "    docs,\n",
    "    model=\"text-embedding-3-large\",\n",
    "    batch_size=64,\n",
    "    dimensions=None,  # omit to use the model default size\n",
    ")\n",
    "\n",
    "# Print a quick report:\n",
    "for r in lengths[:26]:\n",
    "    print(r[\"chunk_index\"], r[\"chunk_id\"], r[\"embedding_length\"])\n",
    "\n",
    "# If you just want to verify they're consistent:\n",
    "unique_lengths = sorted({r[\"embedding_length\"] for r in lengths})\n",
    "print(\"Unique embedding lengths:\", unique_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d958345-8089-47c5-8a7c-406b15e6f616",
   "metadata": {},
   "source": [
    "### Function to put this all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d147aaac-4da1-4a03-a91a-a7c717017409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def build_breadcrumbs(md: dict) -> str:\n",
    "    candidates = [\n",
    "        md.get(\"Header 1\") or md.get(\"h1\"),\n",
    "        md.get(\"Header 2\") or md.get(\"h2\"),\n",
    "        md.get(\"Header 3\") or md.get(\"h3\"),\n",
    "    ]\n",
    "    parts = [p.strip() for p in candidates if p and isinstance(p, str) and p.strip()]\n",
    "    return \" > \".join(parts)\n",
    "\n",
    "\n",
    "def process_markdown_to_embedded_jsonl(\n",
    "    markdown_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    *,\n",
    "    doc_id: str,\n",
    "    version_id: str,\n",
    "    published_year: int,\n",
    "    language: str = \"et\",\n",
    "    embedding_model: str = \"text-embedding-3-large\",\n",
    "    batch_size: int = 64,\n",
    "    dimensions: Optional[int] = None,\n",
    ") -> None:\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not found in environment\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    md_path = Path(markdown_path)\n",
    "    out_path = Path(output_jsonl_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    source_file = md_path.name\n",
    "    source_path = str(md_path)\n",
    "\n",
    "    markdown_text = md_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # 1) Context-aware splitting\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    splits = splitter.split_text(markdown_text)\n",
    "\n",
    "    # 2) Build flat records\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    for i, d in enumerate(splits):\n",
    "        md_in = dict(d.metadata or {})\n",
    "        breadcrumbs = build_breadcrumbs(md_in)\n",
    "\n",
    "        text = d.page_content\n",
    "        text_hash = sha256_text(text)\n",
    "        chunk_id = f\"{doc_id}::{version_id}::chunk_{i:04d}\"\n",
    "\n",
    "        # Typesense requires a top-level \"id\"\n",
    "        ts_id = sha256_text(f\"{chunk_id}::{text_hash}\")\n",
    "\n",
    "        records.append({\n",
    "            \"id\": ts_id,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"version_id\": version_id,\n",
    "            \"published_year\": published_year,\n",
    "            \"language\": language,\n",
    "            \"source_file\": source_file,\n",
    "            \"source_path\": source_path,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"breadcrumbs\": breadcrumbs,\n",
    "            \"text_hash\": text_hash,\n",
    "            \"text\": text,\n",
    "        })\n",
    "\n",
    "    # 3) Embed + write JSONL\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for start in range(0, len(records), batch_size):\n",
    "            batch = records[start:start + batch_size]\n",
    "            inputs = [r[\"text\"] for r in batch]\n",
    "\n",
    "            req: Dict[str, Any] = {\n",
    "                \"model\": embedding_model,\n",
    "                \"input\": inputs,\n",
    "                \"encoding_format\": \"float\",\n",
    "            }\n",
    "            if dimensions is not None:\n",
    "                req[\"dimensions\"] = dimensions\n",
    "\n",
    "            resp = client.embeddings.create(**req)\n",
    "            if len(resp.data) != len(batch):\n",
    "                raise RuntimeError(\"Embedding count mismatch\")\n",
    "\n",
    "            for record, item in zip(batch, resp.data):\n",
    "                emb = item.embedding\n",
    "                record_out = {\n",
    "                    **record,\n",
    "                    \"embedding_model\": embedding_model,\n",
    "                    \"embedding_dimensions\": len(emb),\n",
    "                    \"embedding\": emb,\n",
    "                }\n",
    "                f.write(json.dumps(record_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Wrote {len(records)} Typesense-ready documents to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04bcf310-746e-42c1-82de-1770f7610f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote 26 Typesense-ready documents to: data/t2d_guideline_ee/30_embeddings/2-tuubi-diabeedi-diagnostika-ravi.jsonl\n"
     ]
    }
   ],
   "source": [
    "process_markdown_to_embedded_jsonl(\n",
    "    markdown_path=\"data/t2d_guideline_ee/20_normalized_md/2-tuubi-diabeedi-diagnostika-ravi.md\",\n",
    "    output_jsonl_path=\"data/t2d_guideline_ee/30_embeddings/2-tuubi-diabeedi-diagnostika-ravi.jsonl\",\n",
    "    doc_id=\"t2d_guideline_ee\",\n",
    "    version_id=\"RJ-E_51.1-2021\",\n",
    "    published_year=2021,\n",
    "    language=\"et\",\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "80ddb272-5fc2-40d8-8e59-a9f5fab134e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def build_breadcrumbs(md: dict) -> str:\n",
    "    candidates = [\n",
    "        md.get(\"Header 1\") or md.get(\"h1\"),\n",
    "        md.get(\"Header 2\") or md.get(\"h2\"),\n",
    "        md.get(\"Header 3\") or md.get(\"h3\"),\n",
    "    ]\n",
    "    parts = [p.strip() for p in candidates if p and isinstance(p, str) and p.strip()]\n",
    "    return \" > \".join(parts)\n",
    "\n",
    "\n",
    "def process_markdown_to_jsonl(\n",
    "    markdown_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    *,\n",
    "    doc_id: str,\n",
    "    version_id: str,\n",
    "    published_year: int,\n",
    "    language: str = \"et\",\n",
    ") -> None:\n",
    "    md_path = Path(markdown_path)\n",
    "    out_path = Path(output_jsonl_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    source_file = md_path.name\n",
    "    source_path = str(md_path)\n",
    "\n",
    "    markdown_text = md_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # 1) Context-aware splitting\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    splits = splitter.split_text(markdown_text)\n",
    "\n",
    "    # 2) Build flat records and write JSONL\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, d in enumerate(splits):\n",
    "            md_in = dict(d.metadata or {})\n",
    "            breadcrumbs = build_breadcrumbs(md_in)\n",
    "\n",
    "            text = d.page_content\n",
    "            text_hash = sha256_text(text)\n",
    "            chunk_id = f\"{doc_id}::{version_id}::chunk_{i:04d}\"\n",
    "\n",
    "            # Stable Typesense document id\n",
    "            ts_id = sha256_text(f\"{chunk_id}::{text_hash}\")\n",
    "\n",
    "            record = {\n",
    "                \"id\": ts_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"version_id\": version_id,\n",
    "                \"published_year\": published_year,\n",
    "                \"language\": language,\n",
    "                \"source_file\": source_file,\n",
    "                \"source_path\": source_path,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"breadcrumbs\": breadcrumbs,\n",
    "                \"text_hash\": text_hash,\n",
    "                \"text\": text,\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Wrote {len(splits)} JSONL chunks (no embeddings) to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f91d612-63f5-4b27-bf79-1ae96c0164a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote 26 JSONL chunks (no embeddings) to: data/t2d_guideline_ee/30_chunks/2-tuubi-diabeedi-diagnostika-ravi.jsonl\n"
     ]
    }
   ],
   "source": [
    "process_markdown_to_jsonl(\n",
    "    markdown_path=\"data/t2d_guideline_ee/20_normalized_md/2-tuubi-diabeedi-diagnostika-ravi.md\",\n",
    "    output_jsonl_path=\"data/t2d_guideline_ee/30_chunks/2-tuubi-diabeedi-diagnostika-ravi.jsonl\",\n",
    "    doc_id=\"t2d_guideline_ee\",\n",
    "    version_id=\"RJ-E_51.1-2021\",\n",
    "    published_year=2021,\n",
    "    language=\"et\",\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8fbe2c-2ce0-41d0-b57b-6460a6a90f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "def process_markdown_to_jsonl(\n",
    "    file_path: str,\n",
    "    output_file_path: str,\n",
    "    guideline_name: str,\n",
    "    # Manual Configuration Fields\n",
    "    class_name: str = \"t2dm_guideline_ee\",\n",
    "    version_id: str = \"1.0\",\n",
    "    published_year: int = 2021,\n",
    "    language: str = \"et\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Parses a markdown file into Weaviate-ready JSONL with integer IDs \n",
    "    and combined search text for context-aware retrieval.\n",
    "    \"\"\"\n",
    "    md_path = Path(file_path)\n",
    "    \n",
    "    # 1. Read Markdown File\n",
    "    markdown_text = md_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # 2. Configure Splitter (Context-Aware)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # 3. Split Text\n",
    "    splits = splitter.split_text(markdown_text)\n",
    "\n",
    "    # 4. Write to JSONL\n",
    "    # We use 'w' to overwrite or 'a' to append. 'w' is safer for fresh runs.\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, doc in enumerate(splits):\n",
    "            \n",
    "            # A. Build Breadcrumbs (Hierarchy)\n",
    "            # MarkdownHeaderTextSplitter stores headers in metadata\n",
    "            md = doc.metadata\n",
    "            breadcrumbs_list = [\n",
    "                md.get(\"Header 1\"),\n",
    "                md.get(\"Header 2\"),\n",
    "                md.get(\"Header 3\")\n",
    "            ]\n",
    "            # Filter out None values and join with \" > \"\n",
    "            breadcrumbs = \" > \".join([h for h in breadcrumbs_list if h])\n",
    "            \n",
    "            # B. Prepare Content Fields\n",
    "            clean_text = doc.page_content\n",
    "            \n",
    "            # The \"Context-Enriched\" field for the Embedding Model\n",
    "            # Combines hierarchy + content\n",
    "            combined_search_text = f\"{breadcrumbs}\\n{clean_text}\" if breadcrumbs else clean_text\n",
    "\n",
    "            # C. Construct the Object\n",
    "            record = {\n",
    "                \"class\": class_name,\n",
    "                \"properties\": {\n",
    "                    \"chunk_id\": i,  # Simple Integer (0, 1, 2...)\n",
    "                    \"source\": guideline_name,\n",
    "                    \"version_id\": version_id,\n",
    "                    \"year\": published_year,\n",
    "                    \"language\": language,\n",
    "                    \"breadcrumbs\": breadcrumbs,\n",
    "                    \"text\": clean_text,          # Clean text for LLM/Reading\n",
    "                    \"search_text\": combined_search_text # Enriched text for Vectorizing\n",
    "                }\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully converted {len(splits)} chunks.\")\n",
    "    print(f\"üìÇ Output saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a087c2-e2c2-46c2-93af-84c9f8d4dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully converted 26 chunks.\n",
      "üìÇ Output saved to: data/t2d_guideline_ee/30_chunks/2-tuubi-diabeedi-diagnostika-ravi.jsonl\n"
     ]
    }
   ],
   "source": [
    "process_markdown_to_jsonl(\n",
    "    file_path=\"data/t2d_guideline_ee/20_normalized_md/2-tuubi-diabeedi-diagnostika-ravi.md\",\n",
    "    output_file_path=\"data/t2d_guideline_ee/30_chunks/2-tuubi-diabeedi-diagnostika-ravi.jsonl\",\n",
    "    guideline_name=\"2. t√º√ºbi diabeedi diagnostika ja ravi\",\n",
    "    class_name= \"t2dm_guideline_ee\",\n",
    "    version_id= \"RJ-E_51.1-2021\",\n",
    "    published_year= 2021,\n",
    "    language= \"et\"\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691dff6f-9027-4b74-8c15-a7def374f707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
